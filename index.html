<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | Fall 2020: CS 4476 Computer Vision</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Mask Detection</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Rithik Gavvala, Mithil Verma, Nandin Padheriya, Vamsi Desu, and Sahiti Baddula</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2020 CS 4476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>

The goal of our project is to create a model that detects faces that are wearing face masks.  The system input will simply be various instances of wearing face masks -- some incorrect and some correct. Thus, even if the person has  face mask on but it is incorrectly being worn, the system should put up a red flag. The desired input is a picture of an individual’s face. The desired output should be either fully wearing a mask, partially wearing a mask (not covering both mouth and nose), or not wearing a mask. In the case of a mask being worn correctly, a green boundary will show up around the face. In the case of a mask being worn incorrectly or not at all however, a red boundary will show up.
<br><br>
<!-- figure -->
<h3>Teaser figure</h3>
<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="teaser.png">
</div>

<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
During the COVID pandemic, many health organizations have urged people to wear facial coverings or masks to prevent the spread of coronavirus. This has led to many offices and other establishments that require in-person activity to enforce wearing masks. Our system will enable establishments to determine whether a person is wearing a mask. To solve this problem, we will be using convolutional neural networks, which consists of multiple of convolutional layers, to classify those who are wearing masks and those who are not.
<br><br>
<!-- Approach -->
<h3>Approach</h3>
When we first retrieved the data, there were roughly 6 different columns, where the name represented the name of the image in the training set, x1 through y2 represented the position/coordinates of the person's face who was wearing the mask, and the classname printed symbols that depicted whether the person was wearing a mask, not wearing a mask, wearing a mask incorrectly etc. In order to start on cleaning the data we first decided to  change the values in the image name column so that the values were now pointing to the actual file path of the image. Thus, calling and loading image would be much simpler as it would simply take a query to the name column in order to read in an image. <br>
<br>
Next we noticed that many of the image names in the data frame were being repeated. The reason for this was because many images had many different people present in them. Thus, each separate image entry had coordinates for a separate person wearing a separate mask, in order to simplify the data frame, we thus, went through the entries and appended the coordinates present in the same image, into one entry, under a column called commonPosition. By doing so, it was a lot easier for us to reference one image at a time since now, each row simply catered to the features of one individual image. Since we had this new accumulation of coordinate points, we were able to drop our x1 through y2 columns. <br><br>

Lastly, we saw that the dataset had a random array of labels foreach image. Even though there were labels such as "face_with_mask_incorrect", "face_with_mask", and "gas_mask" (which were somewhat related to our topic since they still covered the nose and mouth) there were also labels such as "mask_colorful" and "hat" which did not make any sense to us. Thus, we kept it simple and bundled all labels that were of use to us into one collective label called "face_with_mask" and certain other ones into a category known as "mask_incorrect". Simplifying the labels up tot his point made it a lot easier for us to go ahead and run our Sequencer model in order to properly train our dataset and later on fit it to images we would try to predict.

<br><br>
Our first approach was to build a simple convolutional neural network. That network consisted of one convolutional layer, two pooling layers, and one dense layer(regular densely-connected NN layer). When training on model, we saw an accuracy of 85% and loss of .38. This was a great starting point for our project, as we understood that convolutional neural networks were best way to classify for our dataset. However, our group knew that the model still has some improvements that could be made. A model with 85% accuracy would not be an effective model in the real world, as businesses would have to inquire about false positives or false negatives for around fifteen out of every hundred images. Extracting features and tuning our hyperparameters were our focus from this point. Next, we added another convolutional layer and dense layer. We were expecting the model to learn more features from our training data set and have a higher accuracy rate. However, the model had only a 85% accuracy rate, which means there was no improvement from the earlier model. Therefore, we decided to focus on other parameters. We then changed our optimizer from Adadelta to Adam and that resulted in an accuracy of 86%. This also resulted in little to no improvement. Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients. We were puzzled as to how to achieve a better accuracy. However, we increased the number of epochs we ran and decreased our batch size. These changes saw a large improvement to our accuracy rate, resulting in 96% accuracy. Adjusting batch size and epochs helped minimize our loss because the model trains more and more with a larger epoch size and usually results in better fitted data. However, overfitting may be an issue, so we also experimented with the number of epochs to avoid overfitting.
<br> <br>
We then focused on building our model out. We had 2 convolution layers for extracting features in our neural network. This network also has two dense layers and two pooling layers. We had some trouble understanding how to work with each layer, but we experimented with these models to get the final result.






<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
In order to conduct our experiments we primarily worked with the Kaggle dataset which consists of 6024 images.
<br><br>
We primarily used loss as an indicator to understand how well our approach was doing. In our figure, we show the loss decreasing as our epochs increase. In addition, the bar plot shows the number of pictures with masks versos the number of pictures that don’t contain masks. The first baseline is the random model, which randomly picks between the two options. This gives us an accuracy of 50%. 
In the image below, we have an example of model trained with more than just two labels. This is an example of how inaccurate our model can be without finetuning any parameters. Our first goal was to beat this model as this serves as a baseline of a model that is essentially guessing the result.Afterwards, we focused on building a convolutional neural network. After some optimizations, we built for sequential model with 2 convolutional layers. These optimizations involved changing our optimizers, adding more convolutional and dense layers to our network, and increasing number of epochs. We saw that increasing the number of epochs was key, as more epochs allows for more fitting on the training data. When tuning the parameters, we also used an Adam optimizer to optimize the learning rate and decay to get the best accuracy for our model. Our accuracy is currently at 96%.
<div style="text-align: center;">
  <img style="height: 200px;" alt="" src="badmodel.png">
  </div>
  <br><br>Figure 1: Example of model without finetuned hyperparameters

  




<br>

<br><br>

<!-- Main Results Figure --> 
<div>
  <div style="text-align: center;">
    <img style="height: 200px;" alt="" src="barchart1.png">
    </div>
    Figure 2: Data distribution
    <br><br>
    <div style="text-align: center;">
      <img style="height: 200px;" alt="" src="table.png">
      </div>
      Figure 3: Snapshot of training data set
      <br><br>
      <div style="text-align: center;">
        <img style="height: 200px;" alt="" src="table2.png">
        </div>
        Figure 4: Snapshot of bounding box data for each image 
        <br><br>
        <div style="text-align: center;">
          <img style="height: 200px;" alt="" src="trainingloss.png">
          </div>
          Figure 5: Training loss for final model
          <br><br>
       
            <br><br>
</div>

<br><br>

<!-- Results -->
<h3>Qualitative Results</h3>
<br><br>
<div style="text-align: center;">
  <img style="height: 200px;" alt="" src="detection2.png">
  </div>
  Figure 6: Result from a model trained with less epochs and yielded lower accuracy
  <br><br>
  <div style="text-align: center;">
    <img style="height: 200px;" alt="" src="detection3.png">
    </div>
    Figure 7: Another result from a model trained with less epochs and yielded lower accuracy
    <br><br>
    <div style="text-align: center;">
      <img style="height: 200px;" alt="" src="detection4.png">
      </div>
      Figure 8: Result from the final model 
      <br><br>
     
<!-- Main Results Figure --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="boundingbox.png">
</div>
Figure 8: Example of an accurate prediction from current model
<br><br>

<h3>Conclusion</h3>
The motivation behind this problem is to make sure people are wearing masks at the appropriate times to prevent the spread of coronavirus. Our model is able to accurately detect faces that are wearing face masks. Our system is able to identify someone wearing a mask, partially wearing a mask (not covering both mouth and nose), or not wearing a mask. We used a sequential model that has two convolutional layers. Currently, our model is at a an accuracy of 96%. We can continue to experiment with optimizers and the number of layers in our model while trying not to overfit the data. Additionally, we can try implementing our system in a real world setting and determine if there are any issues with our system. In the future, we can also work to implement a end-to-end notification system that alerts businesses if a person in their store is not wearing a mask. This can be done through building a mobile app with push notification ability. 

<h3>References</h3>
<ul>
  <li>
    https://www.kaggle.com/wobotintelligence/face-mask-detection-dataset
  </li>
  <li>
    https://arxiv.org/pdf/2008.08016.pdf
  </li>
  <li>
    https://keras.io/api/optimizers/
  </li>
</ul>
<br><br>


  <hr>
  <footer> 
  <p>©Rithik Gavvala, Mithil Verma, Nandin Padheriya, Vamsi Desu, and Sahiti Baddula </p>
  </footer>
</div>
</div>

<br><br>

</body></html>
